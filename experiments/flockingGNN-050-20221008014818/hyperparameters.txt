2022/10/08 01:48:18

n_agents = 50
n_agents_max = 50
n_sim_points = 1
comm_radius = 2.0
repel_dist = 1.0
n_train = 400
n_valid = 20
n_test = 20
duration = 2.0
sampling_time = 0.01
init_geometry = circular
init_vel_value = 3.0
init_min_dist = 0.1
accel_max = 10.0
n_realizations = 10
use_gpu = True

optimization_algorithm = ADAM
learning_rate = 0.0005
beta_1 = 0.9
beta_2 = 0.999
loss_function = <class 'torch.nn.modules.loss.MSELoss'>
trainer = <class 'alegnn.modules.training.TrainerFlocking'>
evaluator = <function evaluate_flocking at 0x000002038BAD6430>
prob_expert = 0.993
n_epochs = 30
batch_size = 20
do_learning_rate_decay = False
learning_rate_decay_rate = 0.9
learning_rate_decay_period = 1
validation_interval = 5

name = DAGNN1Ly
archit = <class 'alegnn.modules.architectures_time.AggregationGNN_DB'>
device = cuda:0
dim_features = [6]
n_filter_taps = []
bias = True
nonlinearity = <class 'torch.nn.modules.activation.Tanh'>
pooling_function = <class 'alegnn.utils.graph_ML.NoPool'>
pooling_size = []
dim_readout = [64, 2]
dim_edge_features = 1
n_exchanges = 1

do_print = True
do_logging = False
do_save_vars = True
do_figs = True
save_dir = experiments\flockingGNN-050-20221008014818
print_interval = 1
fig_size = 5
line_width = 2
marker_shape = o
marker_size = 3
video_speed = 0.5
n_videos = 3

n_agents_test = [50]

name = DAGNN1LyG00
this_optimization_algorithm = ADAM
this_trainer = <class 'alegnn.modules.training.TrainerFlocking'>
this_evaluator = <function evaluate_flocking at 0x000002038BAD6430>
this_learning_rate = 0.0005
this_beta_1 = 0.9
this_beta_2 = 0.999

cost_opt_full050R00 = 51.28082228962901
cost_opt_end0050R00 = 0.0036797756037821408

